{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0510 14:26:37.224534 139910674257728 __init__.py:307] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W0510 14:26:37.244841 139910674257728 __init__.py:334] Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ConfigProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-27ed47965afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ConfigProto'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = K.tensorflow_backend._get_available_gpus()\n",
    "print(\"GPUs found: {}\".format(len(gpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.chase import load_data, animate_stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all, ds_all_centered, datasets, datasets_centered, ds_counts = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function to sample batches of contiguous sequences from a given dataset\n",
    "# This one will safely avoid creating sequences that span the boundary between\n",
    "# two different datasets\n",
    "def gen_batches_safe(data, ds_counts, batch_size, seq_len, center=False):\n",
    "    ds_offsets = np.zeros_like(ds_counts)\n",
    "    ds_offsets[1:] = np.cumsum(ds_counts[:-1])\n",
    "    \n",
    "    batch_idxs = []\n",
    "    for ds_len,ds_offset in zip(ds_counts, ds_offsets):\n",
    "        ds_batch_idxs = np.arange(ds_len-seq_len).repeat(seq_len).reshape(-1,seq_len) + np.arange(seq_len)\n",
    "        batch_idxs.append(ds_batch_idxs + ds_offset)\n",
    "    \n",
    "    batch_idxs = np.concatenate(batch_idxs)\n",
    "    \n",
    "    nbatch = batch_idxs.shape[0]//batch_size\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(batch_idxs)\n",
    "        for ibatch in range(nbatch):\n",
    "            batch = data[batch_idxs[ibatch*batch_size:(ibatch+1)*batch_size]]\n",
    "            yield batch, None\n",
    "            #yield batch, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "class BiasLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, bias_init='zeros', bias_std=0.01, **kwargs):\n",
    "        self.bias_init = bias_init\n",
    "        self.bias_std = bias_std\n",
    "        super(BiasLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        if self.bias_init == 'zeros':\n",
    "            init = 'zeros'\n",
    "        elif self.bias_init == 'normal':\n",
    "            init = initializers.RandomNormal(stddev=self.bias_std)\n",
    "        \n",
    "        self.bias = self.add_weight(name='bias', \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=init,\n",
    "                                      trainable=True)\n",
    "        super(BiasLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        #return K.dot(x, self.kernel)\n",
    "        return x + self.bias\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return (input_shape[0], self.output_dim)\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotationLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, theta, dim=3, learning_phase_only=True, **kwargs):\n",
    "        self.theta = theta\n",
    "        self.vec_dim = dim\n",
    "        \n",
    "        if dim == 3:\n",
    "            self.R = K.constant([[1,0,0],[0,1,0],[0,0,0]])*tf.cos(theta) \\\n",
    "                       + K.constant([[0,-1,0],[1,0,0],[0,0,0]])*tf.sin(theta) \\\n",
    "                       + K.constant([[0,0,0],[0,0,0],[0,0,1]])\n",
    "        elif dim == 2:\n",
    "            self.R = K.constant([[1,0],[0,1]])*tf.cos(theta) \\\n",
    "                       + K.constant([[0,-1],[1,0]])*tf.sin(theta)\n",
    "        \n",
    "        self.uses_learning_phase = learning_phase_only\n",
    "        \n",
    "        super(RotationLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if self.uses_learning_phase:\n",
    "            return K.in_train_phase(K.dot(x, self.R), x, training=training)\n",
    "        else:\n",
    "            return K.dot(x, self.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_seq_ae(X, seq_len, latent_dim=32, n_layers=2, n_units=32, use_dense=True, kl_weight=0, resolution=3e-3, do_rotations=True, extrap_len=8):\n",
    "    K.clear_session()\n",
    "    \n",
    "    n_vtx = X.shape[1]\n",
    "    \n",
    "    encoder_input = layers.Input((seq_len, n_vtx, 3))\n",
    "    H = encoder_input\n",
    "    \n",
    "    #H = layers.Flatten()(H)\n",
    "    H = layers.Reshape((seq_len, n_vtx*3))(H)\n",
    "    \n",
    "    for i in range(n_layers-1):\n",
    "        H = layers.LSTM(n_units, return_sequences=True)(H)\n",
    "    \n",
    "    if use_dense:\n",
    "        H = layers.LSTM(n_units, return_sequences=False)(H)\n",
    "\n",
    "        z_mean = layers.Dense(latent_dim, name='z_mean')(H)\n",
    "        z_log_var = layers.Dense(latent_dim, name='z_log_var')(H)\n",
    "    else:\n",
    "        H = layers.CuDNNLSTM(n_units, return_sequences=True)(H)\n",
    "        \n",
    "        H = layers.CuDNNLSTM(2*latent_dim, return_sequences=False)(H)\n",
    "\n",
    "        z_mean = layers.Lambda(lambda x: x[:,:latent_dim], name='z_mean')(H)\n",
    "        z_log_var = layers.Lambda(lambda x: x[:,latent_dim:], name='z_log_var')(H)\n",
    "        \n",
    "    z_sample = layers.Lambda(sample_z, output_shape=(latent_dim,), name='z_sample')([z_mean, z_log_var])\n",
    "    \n",
    "    encoder_output = [z_sample, z_mean, z_log_var]\n",
    "    \n",
    "    encoder = Model(encoder_input, encoder_output)\n",
    "    \n",
    "    \n",
    "    decoder_input = layers.Input((latent_dim,))\n",
    "    H = decoder_input\n",
    "    \n",
    "    if use_dense:\n",
    "        H = layers.Dense(n_units, activation='relu')(H)\n",
    "\n",
    "    H = layers.RepeatVector(seq_len)(H)\n",
    "\n",
    "    for i in range(n_layers-1):\n",
    "        H = layers.CuDNNLSTM(n_units, return_sequences=True)(H)\n",
    "\n",
    "    H = layers.CuDNNLSTM(n_vtx*3, return_sequences=True)(H)\n",
    "\n",
    "    H = layers.Reshape((seq_len, n_vtx, 3))(H)\n",
    "    decoder_output = H\n",
    "\n",
    "    decoder = Model(decoder_input, decoder_output)\n",
    "    \n",
    "    \n",
    "    auto_input = layers.Input((seq_len,n_vtx,3))\n",
    "    \n",
    "    H = auto_input\n",
    "    \n",
    "    if do_rotations:\n",
    "        theta = K.cast(K.learning_phase(),'float')*K.random_uniform((1,), 0, 2*np.pi)\n",
    "        H = RotationLayer(theta)(H)\n",
    "    \n",
    "    auto_z, auto_mean, auto_log_var = encoder(H)\n",
    "    H = decoder(auto_z)\n",
    "    \n",
    "    if do_rotations:\n",
    "        H = RotationLayer(-theta)(H)\n",
    "    \n",
    "    auto_output = H\n",
    "\n",
    "    auto = Model(auto_input, auto_output)\n",
    "    \n",
    "    \n",
    "    auto.hp_resolution = K.variable(resolution)\n",
    "    \n",
    "    ae_loss = 0.5*K.mean(K.sum(K.square(auto_input - auto_output), axis=-1))\n",
    "    auto.add_loss(ae_loss/K.square(auto.hp_resolution))\n",
    "\n",
    "    if kl_weight:\n",
    "        kl_loss = -0.5*K.mean(K.sum(1 + auto_log_var - K.square(auto_mean) - K.exp(auto_log_var), axis=-1))\n",
    "\n",
    "        auto.hp_kl_weight = K.variable(kl_weight)\n",
    "        auto.add_loss(auto.hp_kl_weight * kl_loss)\n",
    "\n",
    "    auto.compile(optimizer='adam')\n",
    "    \n",
    "    def calc_shift_residual(args):\n",
    "        x0, x1 = args\n",
    "        \n",
    "        #return x0[:,seq_len//2:]-x1[:,:-seq_len//2]\n",
    "        return x0[:,extrap_len:] - x1[:,:-extrap_len]\n",
    "    shift_residual = layers.Lambda(calc_shift_residual, name='shift_residual')\n",
    "    \n",
    "    def mk_continuizer(nstep):\n",
    "        continuizer_input = layers.Input((seq_len, n_vtx, 3))\n",
    "        #continuizer_x0 = continuizer_input\n",
    "        #continuizer_z0, continuizer_z0_mean, continuizer_z0_logvar = encoder(continuizer_x0)\n",
    "        #continuizer_x0_clean = decoder(continuizer_z0_mean)\n",
    "        _, continuizer_z0, _ = encoder(continuizer_input)\n",
    "        continuizer_x0 = decoder(continuizer_z0)\n",
    "        \n",
    "        continuizer_z1 = BiasLayer()(continuizer_z0)\n",
    "        continuizer_x1 = decoder(continuizer_z1)\n",
    "        \n",
    "        continuizer_output = [continuizer_x1, continuizer_z1]\n",
    "        continuizer = Model(continuizer_input, continuizer_output)\n",
    "        \n",
    "        curve = np.exp(-np.arange(seq_len-1-extrap_len,-1,-1)/((seq_len-extrap_len)/8))\n",
    "        curve /= curve.sum()\n",
    "        curve = K.constant(curve.reshape((1,seq_len-extrap_len,1,1)))\n",
    "        \n",
    "        continuizer.add_loss(0.5/K.square(auto.hp_resolution)*\\\n",
    "                             K.mean(K.sum(curve*K.square(shift_residual([continuizer_x0, continuizer_x1])), axis=-1)))\n",
    "        \n",
    "        decoder.trainable = False\n",
    "        encoder.trainable = False\n",
    "        continuizer.compile(optimizer='adam')\n",
    "        #continuizer.compile(optimizer='rmsprop')\n",
    "        decoder.trainable = True\n",
    "        encoder.trainable = True\n",
    "        return continuizer\n",
    "    \n",
    "    return encoder, decoder, auto, mk_continuizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len      = 128\n",
    "latent_dim   = 256\n",
    "n_layers     = 3 #2\n",
    "n_units      = 384 #256\n",
    "use_dense    = True\n",
    "kl_weight    = 1 #1e-2\n",
    "resolution   = 3e-1 #1e-2\n",
    "lr           = 3e-4\n",
    "do_rotations = True\n",
    "extrap_len   = seq_len//2\n",
    "#do_shift     = False\n",
    "#do_inplace   = False\n",
    "\n",
    "encoder, decoder, auto, mk_continuizer = mk_seq_ae(ds_all, seq_len=seq_len, latent_dim=latent_dim,\n",
    "                                   n_units=n_units, n_layers=n_layers,\n",
    "                                  use_dense=use_dense, kl_weight=kl_weight,\n",
    "                                  resolution=resolution, do_rotations=do_rotations, extrap_len=extrap_len)\n",
    "continuizer = mk_continuizer(1)\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "auto.summary()\n",
    "\n",
    "K.set_value(auto.optimizer.lr, lr)\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights = True\n",
    "load_weights = False\n",
    "\n",
    "if save_weights:\n",
    "    print(\"Saving weights\")\n",
    "    encoder.save_weights('LSTM_enc_weights.h5')\n",
    "    decoder.save_weights('LSTM_dec_weights.h5')\n",
    "    auto.save_weights('LSTM_auto_weights.h5')\n",
    "if load_weights:\n",
    "    print(\"Loading weights\")\n",
    "    encoder.load_weights('seq_vae_enc_weights.h5')\n",
    "    decoder.load_weights('seq_vae_dec_weights.h5')\n",
    "    auto.load_weights('seq_vae_auto_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 512\n",
    "\n",
    "nstep = sum([c-seq_len for c in ds_counts])//batch_size\n",
    "\n",
    "K.set_value(auto.optimizer.lr, 1e-4)\n",
    "K.set_value(auto.hp_kl_weight, 2e-4)\n",
    "\n",
    "try:\n",
    "#     auto.fit_generator(gen_batches_safe(ds_all_centered, ds_counts, batch_size, seq_len),\n",
    "#                        steps_per_epoch=nstep, epochs=epochs, verbose=2)\n",
    "    auto.fit(gen_batches_safe(ds_all_centered, ds_counts, batch_size, seq_len), steps_per_epoch=nstep, epochs=epochs, verbose=2)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted.\")\n",
    "\n",
    "print(\"Updating loss history\")\n",
    "loss_history.extend(auto.history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nskip = 2\n",
    "xepochs = np.arange(len(loss_history))+1\n",
    "plt.plot(xepochs[nskip:], loss_history[nskip:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze | grep tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mariel' # string used to define filename of saved model\n",
    "\n",
    "auto.save(model_name + '-decoder.hdf5')\n",
    "cmd = 'tensorflowjs_converter --input_format keras ' + model_name + '-decoder.hdf5 ' + model_name + '-decoder-js --rapid'\n",
    "\n",
    "# run `pip install tensorflowjs` first\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check autoencoder reconstruction performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder (red) tries to imitate the real Mariel (pink):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_start = np.random.randint(0,len(ds_all_centered)-seq_len)\n",
    "print(\"Seeding with frame {}\".format(index_start))\n",
    "xtest = ds_all_centered[index_start:index_start+seq_len]\n",
    "xpred = auto.predict(np.expand_dims(xtest,axis=0))[0]\n",
    "HTML(animate_stick(xtest,ghost=xpred, ghost_shift=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try some variations by adding noise to latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ztest, _ = encoder.predict(np.expand_dims(xtest,axis=0))\n",
    "xproj = decoder.predict(ztest + np.random.normal(0,0.25,latent_dim))[0]\n",
    "HTML(animate_stick(xtest, ghost=xproj, ghost_shift=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ztest, _ = encoder.predict(np.expand_dims(xtest,axis=0))\n",
    "xproj = decoder.predict(ztest + np.random.normal(0,0.5,latent_dim))[0]\n",
    "HTML(animate_stick(xtest, ghost=xproj, ghost_shift=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ztest, _ = encoder.predict(np.expand_dims(xtest,axis=0))\n",
    "xproj = decoder.predict(ztest + np.random.normal(0,1,latent_dim))[0]\n",
    "HTML(animate_stick(xtest, ghost=xproj, ghost_shift=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try sampling randomly from the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "\n",
    "xgen = decoder.predict(np.random.normal(0,sigma,(1,latent_dim)))[0]\n",
    "HTML(animate_stick(xgen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to project the sequence via regression in latent space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.trainable = False\n",
    "decoder.trainable = False\n",
    "K.set_value(continuizer.trainable_weights[0], np.random.normal(0,0.1/latent_dim,latent_dim))\n",
    "#K.set_value(continuizer.trainable_weights[0], np.zeros(latent_dim))\n",
    "K.set_value(continuizer.optimizer.lr, 4e-3)\n",
    "encoder.trainable = True\n",
    "decoder.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.trainable = False\n",
    "decoder.trainable = False\n",
    "for i in range(1000):\n",
    "    closs = continuizer.train_on_batch(np.expand_dims(xproj[-seq_len:],axis=0), None)\n",
    "    if i%500==0:\n",
    "        print(\"i=%d closs=%.2e\"%(i,closs))\n",
    "    if closs < 1e-7:\n",
    "        print(\"Breaking at i=%d, closs=%.2e\"%(i,closs))\n",
    "        break\n",
    "print(closs)\n",
    "print(\"|dz|=%.2e\"%np.sqrt(np.sum(K.get_value(continuizer.trainable_weights[0])**2)))\n",
    "encoder.trainable = True\n",
    "decoder.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.trainable = False\n",
    "decoder.trainable = False\n",
    "plt.hist(K.get_value(continuizer.trainable_weights[0]))\n",
    "print(\"|dz|=%.2e\"%np.sqrt(np.sum(K.get_value(continuizer.trainable_weights[0])**2)))\n",
    "encoder.trainable = True\n",
    "decoder.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnext = continuizer.predict(np.expand_dims(xproj[-seq_len:],axis=0))[0][0]\n",
    "\n",
    "plt.plot(np.mean(np.sum((xnext[:-extrap_len]-xproj[-(seq_len-extrap_len):])**2, axis=-1), axis=-1))\n",
    "HTML(animate_stick(xnext[:-extrap_len], ghost=xproj[-(seq_len-extrap_len):], ghost_shift=0.))\n",
    "\n",
    "#HTML(animate_stick(xnext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xnext = continuizer.predict(np.expand_dims(xproj[-seq_len:],axis=0))[0][0]\n",
    "xproj = np.append(xproj, xnext[-extrap_len:], axis=0)\n",
    "print(xproj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animate_stick(xproj, ghost=ds_all_centered[400:400+len(xproj)], ghost_shift=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xproj[-seq_len-seq_len//2:-seq_len//2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xproj_smooth = xproj.copy()\n",
    "_,z,_ = encoder.predict(np.expand_dims(xproj[-seq_len:], axis=0))\n",
    "xproj_smooth[-seq_len:] = decoder.predict(z)[0]\n",
    "_,z,_ = encoder.predict(np.expand_dims(xproj_smooth[-seq_len-seq_len//2:-seq_len//2], axis=0))\n",
    "xproj_smooth[-seq_len-seq_len//2:-seq_len//2] = decoder.predict(z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animate_stick(xproj_smooth, ghost=ds_all_centered[400:400+len(xproj)], ghost_shift=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "choreo_kernel",
   "language": "python",
   "name": "choreo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
