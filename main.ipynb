{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functions.functions import *\n",
    "from functions.plotting import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('data/mariel_*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = np.random.randint(0,data.full.X.shape[1]-50)\n",
    "print(\"Starting from frame {}...\".format(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animate(data.full.X[:,frame:,:], frames=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animate(data.selected.X[:,frame:,:], frames=50, edges=data.selected.edges, colors='black'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the body with labels for cherry picking edges\n",
    "# plot_labelled_points(data.full.X, frame_idx=11000, text=True, figsize=(50,40), font_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the vanilla autoencoder for poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.selected.X[:,:,:]    # 15 joints\n",
    "# X_train = data.full.X[:,:,:]    # 55 joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train without adding random (x,y) offsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae_nooffset = Autoencoder(n_verts=15, latent_dim=6, n_layers=2, n_units=128, relu=True, dropout=False, add_random_offsets=False)\n",
    "X_T = X_train.transpose((1,0,2))\n",
    "ae_nooffset.model.fit(X_T, X_T, batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoded prediction in blue, real data in black:\n",
    "starting_frame = 16600\n",
    "n_frames = 100\n",
    "predictions = ae_nooffset.get_predictions(X_train, start_frame=starting_frame, n_frames=n_frames)\n",
    "HTML(animate_ghost(data.selected.X[:,starting_frame:,:], predictions, frames=n_frames, edges=data.selected.edges, colors='blue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with adding random (x,y) offsets to the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae_withoffset = Autoencoder(n_verts=15, latent_dim=6, n_layers=2, n_units=128, relu=True, dropout=False, add_random_offsets=False)\n",
    "X_T = X_train.transpose((1,0,2))\n",
    "ae_withoffset.model.fit(X_T, X_T, batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoded prediction in blue, real data in black:\n",
    "starting_frame = 16600\n",
    "n_frames = 100\n",
    "predictions = ae_withoffset.get_predictions(X_train, start_frame=starting_frame, n_frames=n_frames)\n",
    "HTML(animate_ghost(data.selected.X[:,starting_frame:,:], predictions, frames=n_frames, edges=data.selected.edges, colors='blue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Analyze the autoencoder latent space\n",
    "# X = data.selected.X\n",
    "# encoded = []\n",
    "# for idx, i in enumerate(range(X.shape[1])):\n",
    "#   positions = np.expand_dims(X[:,i,:].squeeze(), axis=0)\n",
    "#   encoded.append( ae.encoder.predict(positions) )\n",
    "#   if idx and idx % 10000 == 0:\n",
    "#     print(' * processed', idx, 'of', X.shape[1])\n",
    "# encoded = np.array(encoded).squeeze() # shape = (13463, ae.latent_dim)\n",
    "\n",
    "# # draw the plot\n",
    "# fig = plt.figure(figsize=(20, 14))\n",
    "# ax = p3.Axes3D(fig)\n",
    "# ax.scatter(encoded[:,0], encoded[:,1], encoded[:,2], depthshade=False, alpha=0.3, s=0.5, c=np.arange(len(encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + MDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://raw.githubusercontent.com/omimo/Keras-MDN/master/kmdn/mdn.py\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, merge, concatenate, Dense, LSTM, CuDNNLSTM\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# check tfp version, as tfp causes cryptic error if out of date\n",
    "assert float(tfp.__version__.split('.')[1]) >= 5\n",
    "\n",
    "class MDN(Layer):\n",
    "  '''Mixture Density Network with unigaussian kernel'''\n",
    "  def __init__(self, n_mixes, output_dim, **kwargs):\n",
    "    self.n_mixes = n_mixes\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "    with tf.name_scope('MDN'):\n",
    "      self.mdn_mus    = Dense(self.n_mixes * self.output_dim, name='mdn_mus')\n",
    "      self.mdn_sigmas = Dense(self.n_mixes, activation=K.exp, name='mdn_sigmas')\n",
    "      self.mdn_alphas = Dense(self.n_mixes, activation=K.softmax, name='mdn_alphas')\n",
    "    super(MDN, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.mdn_mus.build(input_shape)\n",
    "    self.mdn_sigmas.build(input_shape)\n",
    "    self.mdn_alphas.build(input_shape)\n",
    "    self.trainable_weights = self.mdn_mus.trainable_weights + \\\n",
    "      self.mdn_sigmas.trainable_weights + \\\n",
    "      self.mdn_alphas.trainable_weights\n",
    "    self.non_trainable_weights = self.mdn_mus.non_trainable_weights + \\\n",
    "      self.mdn_sigmas.non_trainable_weights + \\\n",
    "      self.mdn_alphas.non_trainable_weights\n",
    "    self.built = True\n",
    "\n",
    "  def call(self, x, mask=None):\n",
    "    with tf.name_scope('MDN'):\n",
    "      mdn_out = concatenate([\n",
    "        self.mdn_mus(x),\n",
    "        self.mdn_sigmas(x),\n",
    "        self.mdn_alphas(x)\n",
    "      ], name='mdn_outputs')\n",
    "    return mdn_out\n",
    "\n",
    "  def get_output_shape_for(self, input_shape):\n",
    "    return (input_shape[0], self.output_dim)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "      'output_dim': self.output_dim,\n",
    "      'n_mixes': self.n_mixes,\n",
    "    }\n",
    "    base_config = super(MDN, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  def get_loss_func(self):\n",
    "    def unigaussian_loss(y_true, y_pred):\n",
    "      mix = tf.range(start = 0, limit = self.n_mixes)\n",
    "      out_mu, out_sigma, out_alphas = tf.split(y_pred, num_or_size_splits=[\n",
    "        self.n_mixes * self.output_dim,\n",
    "        self.n_mixes,\n",
    "        self.n_mixes,\n",
    "      ], axis=-1, name='mdn_coef_split')\n",
    "\n",
    "      def loss_i(i):\n",
    "        batch_size = tf.shape(out_sigma)[0]\n",
    "        sigma_i = tf.slice(out_sigma, [0, i], [batch_size, 1], name='mdn_sigma_slice')\n",
    "        alpha_i = tf.slice(out_alphas, [0, i], [batch_size, 1], name='mdn_alpha_slice')\n",
    "        mu_i = tf.slice(out_mu, [0, i * self.output_dim], [batch_size, self.output_dim], name='mdn_mu_slice')\n",
    "        dist = tfp.distributions.Normal(loc=mu_i, scale=sigma_i)\n",
    "        loss = dist.prob(y_true) # find the pdf around each value in y_true\n",
    "        loss = alpha_i * loss\n",
    "        return loss\n",
    "\n",
    "      result = tf.map_fn(lambda  m: loss_i(m), mix, dtype=tf.float32, name='mix_map_fn')\n",
    "      result = tf.reduce_sum(result, axis=0, keepdims=False)\n",
    "      result = -tf.log(result)\n",
    "      result = tf.reduce_mean(result)\n",
    "      return result\n",
    "\n",
    "    with tf.name_scope('MDNLayer'):\n",
    "      return unigaussian_loss\n",
    "\n",
    "class LSTM_MDN:\n",
    "  def __init__(self, n_verts=15, n_dims=3, n_mixes=2, look_back=1, cells=[32,32,32,32], use_mdn=True):\n",
    "    self.n_verts = n_verts\n",
    "    self.n_dims = n_dims\n",
    "    self.n_mixes = n_mixes\n",
    "    self.look_back = look_back\n",
    "    self.cells = cells\n",
    "    self.use_mdn = use_mdn\n",
    "    self.LSTM = CuDNNLSTM if len(gpus) > 0 else LSTM\n",
    "    self.model = self.build_model()\n",
    "    if use_mdn:\n",
    "      self.model.compile(loss=MDN(n_mixes, n_verts*n_dims).get_loss_func(), optimizer='adam', metrics=['accuracy'])\n",
    "    else:\n",
    "      self.model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "  def build_model(self):\n",
    "    i = Input((self.look_back, self.n_verts*self.n_dims))\n",
    "    h = self.LSTM(self.cells[0], return_sequences=True)(i) # return sequences, stateful\n",
    "    h = self.LSTM(self.cells[1], return_sequences=True)(h)\n",
    "    h = self.LSTM(self.cells[2])(h)\n",
    "    h = Dense(self.cells[3])(h)\n",
    "    if self.use_mdn:\n",
    "      o = MDN(self.n_mixes, self.n_verts*self.n_dims)(h)\n",
    "    else:\n",
    "      o = Dense(self.n_verts*self.n_dims)(h)\n",
    "    return Model(inputs=[i], outputs=[o])\n",
    "  \n",
    "  def prepare_inputs(self, X, look_back=2):\n",
    "    '''\n",
    "    Prepare inputs in shape expected by LSTM\n",
    "    @returns:\n",
    "      numpy.ndarray train_X: has shape: n_samples, lookback, verts * dims\n",
    "      numpy.ndarray train_Y: has shape: n_samples, verts * dims\n",
    "    '''\n",
    "    # prepare data for the LSTM_MDN\n",
    "    X = X.swapaxes(0, 1) # reshape to time, vert, dim\n",
    "    n_time, n_verts, n_dims = X.shape\n",
    "    \n",
    "    # validate shape attributes\n",
    "    if n_verts != self.n_verts: raise Exception(' ! got', n_verts, 'vertices, expected', self.n_verts)\n",
    "    if n_dims != self.n_dims: raise Exception(' ! got', n_dims, 'dims, expected', self.n_dims)\n",
    "    if look_back != self.look_back: raise Exception(' ! got', look_back, 'for look_back, expected', self.look_back)\n",
    "    \n",
    "    # lstm expects data in shape [samples_in_batch, timestamps, values]\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(look_back, n_time, 1):\n",
    "      train_X.append( X[i-look_back:i,:,:].reshape(look_back, n_verts * n_dims) ) # look_back, verts * dims\n",
    "      train_Y.append( X[i,:,:].reshape(n_verts * n_dims) ) # verts * dims\n",
    "    train_X = np.array(train_X) # n_samples, lookback, verts * dims\n",
    "    train_Y = np.array(train_Y) # n_samples, verts * dims\n",
    "    return [train_X, train_Y]\n",
    "  \n",
    "  def predict_positions(self, input_X):\n",
    "    '''\n",
    "    Predict the output for a series of input frames. Each prediction has shape (1, y), where y contains:\n",
    "      mus = y[:n_mixes*n_verts*n_dims]\n",
    "      sigs = y[n_mixes*n_verts*n_dims:-n_mixes]\n",
    "      alphas = softmax(y[-n_mixes:])\n",
    "    @param numpy.ndarray input_X: has shape: n_samples, look_back, n_verts * n_dims\n",
    "    @returns:\n",
    "      numpy.ndarray X: has shape: verts, time, dims\n",
    "    '''\n",
    "    predictions = []\n",
    "    for i in range(input_X.shape[0]):\n",
    "      y = self.model.predict( train_X[i:i+1] ).squeeze()\n",
    "      mus = y[:n_mixes*n_verts*n_dims]\n",
    "      sigs = y[n_mixes*n_verts*n_dims:-n_mixes]\n",
    "      alphas = self.softmax(y[-n_mixes:])\n",
    "\n",
    "      # find the most likely distribution then pull out the mus that correspond to that selected index\n",
    "      alpha_idx = np.argmax(alphas) # 0\n",
    "      alpha_idx = 0\n",
    "      predictions.append( mus[alpha_idx*self.n_verts*self.n_dims:(alpha_idx+1)*self.n_verts*self.n_dims] )\n",
    "    predictions = np.array(predictions).reshape(train_X.shape[0], self.n_verts, self.n_dims).swapaxes(0, 1)\n",
    "    return predictions # shape = n_verts, n_time, n_dims\n",
    "    \n",
    "  def softmax(self, x):\n",
    "    ''''Compute softmax values for vector `x`'''\n",
    "    r = np.exp(x - np.max(x))\n",
    "    return r / r.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.selected.X\n",
    "n_verts, n_time, n_dims = X.shape\n",
    "n_mixes = 3\n",
    "look_back = 10\n",
    "\n",
    "lstm_mdn = LSTM_MDN(n_verts=n_verts, n_dims=n_dims, n_mixes=n_mixes, look_back=look_back)\n",
    "train_X, train_Y = lstm_mdn.prepare_inputs(X, look_back=look_back)\n",
    "\n",
    "lstm_mdn = LSTM_MDN(n_verts=n_verts, n_dims=n_dims, n_mixes=n_mixes, look_back=look_back)\n",
    "lstm_mdn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TerminateOnNaN, ModelCheckpoint\n",
    "checkpoint_filepath=\"lstm_nopca_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "lstm_mdn.model.fit(train_X, train_Y, epochs=10, batch_size=64, shuffle=False, callbacks=[checkpoint, TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = lstm_mdn\n",
    "# trained_model.model.load_weights('lstm_nopca_weights-best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how well the model can predict the next frame in the input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how well the model learned the input sequence\n",
    "starting_frame = 1400\n",
    "n_frames = 100 # n frames of time slices to generate\n",
    "output_dims = train_X.shape[2]\n",
    "\n",
    "frames = []\n",
    "\n",
    "test_X = train_X[starting_frame:starting_frame+n_frames] # data to pass into forward prop through the model\n",
    "y_pred = trained_model.model.predict(test_X) # output with shape (n_frames, (output_dims+2) * n_mixes )\n",
    "\n",
    "# partition out the mus, sigs, and mixture weights\n",
    "for i in range(n_frames):\n",
    "    y = y_pred[i].squeeze()\n",
    "    mus = y[:n_mixes*output_dims]\n",
    "    sigs = y[n_mixes*output_dims:n_mixes*output_dims + n_mixes]\n",
    "    alphas = y[-n_mixes:]\n",
    "\n",
    "    # find the most likely distribution - then disregard that number and use the first Gaussian :)\n",
    "    alpha_idx = np.argmax(alphas)\n",
    "    alpha_idx = 0\n",
    "\n",
    "    # pull out the mus that correspond to the selected alpha index\n",
    "    positions = mus[alpha_idx * output_dims:(alpha_idx+1) * output_dims]\n",
    "    frames.append(positions)\n",
    "  \n",
    "frames = np.array(frames)\n",
    "lstm_predictions = np.dstack((frames.T[::3,:],frames.T[1::3,:],frames.T[2::3,:]))\n",
    "HTML(animate_ghost(data.selected.X[:,starting_frame:,:], lstm_predictions[:,:,:], frames=n_frames, edges=data.selected.edges, colors='blue', ghost_shift = 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now generate new sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.model.predict(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 100 # n frames of time slices to generate\n",
    "frames = []\n",
    "\n",
    "seed = np.random.randint(0, len(train_X)-1)\n",
    "x = np.expand_dims(train_X[seed], axis=0)\n",
    "print(' * seeding with', seed)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    y = trained_model.model.predict(x).squeeze()\n",
    "    mus = y[:n_mixes*output_dims]\n",
    "    sigs = y[n_mixes*output_dims:-n_mixes]\n",
    "    alphas = softmax(y[-n_mixes:])\n",
    "\n",
    "    # select the alpha channel to use\n",
    "    alpha_idx = np.argmax(alphas)\n",
    "\n",
    "    # grab the mus and sigs associated with the selected alpha_idx\n",
    "    frame_mus = mus.ravel()[alpha_idx*output_dims : (alpha_idx+1)*output_dims]\n",
    "    frame_sig = sigs[alpha_idx] / 100\n",
    "\n",
    "    # now sample from each Gaussian\n",
    "    positions = [np.random.normal(loc=m, scale=frame_sig) for m in frame_mus]\n",
    "    positions = frame_mus\n",
    "\n",
    "    # add these positions to the results\n",
    "    frames.append(positions)\n",
    "\n",
    "    # pull out a new training example - stack the new result on\n",
    "    # all values after the first from the bottom-most value in the x's\n",
    "    start = x[:,1:,:]\n",
    "    end = np.expand_dims( np.expand_dims(positions, axis=0), axis=0 )\n",
    "    x = np.concatenate((start, end), axis=1)\n",
    "    \n",
    "frames = np.array(frames)\n",
    "lstm_predictions = np.dstack((frames.T[::3,:],frames.T[1::3,:],frames.T[2::3,:]))\n",
    "\n",
    "prompt_plus_generated_seq = np.concatenate((data.selected.X[:,seed:seed+10,:],lstm_predictions), axis=1)\n",
    "\n",
    "HTML(animate(prompt_plus_generated_seq, frames=n_frames, edges=data.selected.edges, colors='black'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT STEPS:\n",
    "- (x,y centering)\n",
    "- augment with random rotations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
