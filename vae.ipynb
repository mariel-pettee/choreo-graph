{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "import networkx as nx # for visualizing graphs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from functions.load_data import MarielDataset, edges\n",
    "from functions.functions import *\n",
    "from functions.modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 10\n",
    "predicted_timesteps = 0\n",
    "data = MarielDataset(seq_len=seq_len, reduced_joints=False, predicted_timesteps=predicted_timesteps)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "print(\"\\nGenerated {:,} batches of shape: {}\".format(len(dataloader), data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = data.seq_len*data.n_dim\n",
    "edge_features = data[0].num_edge_features\n",
    "node_embedding_dim = 25\n",
    "edge_embedding_dim = 4 # number of edge types\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "checkpoint_path = \"weights/checkpoint_53joints_seqlen10.pth\"\n",
    "checkpoint_loaded = False \n",
    "\n",
    "model = VAE(node_features=node_features, \n",
    "            edge_features=edge_features, \n",
    "            hidden_size=hidden_size, \n",
    "            node_embedding_dim=node_embedding_dim,\n",
    "            edge_embedding_dim=edge_embedding_dim,\n",
    "            num_layers=num_layers,\n",
    "            input_size=node_embedding_dim, \n",
    "            output_size=node_features+predicted_timesteps*data.n_dim,\n",
    "           )\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-4, weight_decay=5e-4)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "# gnn_model_summary(model)\n",
    "print(\"Total trainable parameters: {:,}\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: load pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the whole model + weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"weights/seqlen3_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR load the model state into the pre-existing model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss_checkpoint = checkpoint['loss']\n",
    "checkpoint_loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "prediction_to_reconstruction_loss_ratio = 0 # you might want to weight the prediction loss higher to help it compete with the larger prediction seq_len\n",
    "sigma = 0.001 # how to pick sigma?\n",
    "\n",
    "def train(num_epochs):\n",
    "    losses = []\n",
    "    reconstruction_losses = []\n",
    "    prediction_losses = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        average_loss = 0\n",
    "        average_reconstruction_loss = 0\n",
    "        average_prediction_loss = 0\n",
    "        i = 0\n",
    "        batch_inputs = []\n",
    "        batch_outputs = []\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Batches\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad() # reset the gradients to zero\n",
    "            \n",
    "            ### CALCULATE MODEL OUTPUTS\n",
    "            output = model(batch)\n",
    "            batch_inputs.append(batch.x)\n",
    "            batch_outputs.append(output)\n",
    "            \n",
    "            ### CALCULATE LOSS\n",
    "            reconstruction_loss = mse_loss(batch.x.to(device), output[:,:node_features]) # compare first seq_len timesteps\n",
    "            average_reconstruction_loss += reconstruction_loss.item()\n",
    "            batch_loss = reconstruction_loss\n",
    "            if predicted_timesteps > 0: \n",
    "                prediction_loss = mse_loss(batch.y.to(device), output[:,node_features:]) # compare last part to unseen data\n",
    "                batch_loss += prediction_to_reconstruction_loss_ratio*prediction_loss\n",
    "                average_prediction_loss += prediction_loss.item()\n",
    "\n",
    "            ### BACKPROPAGATE\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            average_loss += batch_loss.item()\n",
    "\n",
    "            i += 1\n",
    "            if i >= 3: break # temporary -- for stopping training early\n",
    "                \n",
    "        inputs.append(torch.stack(batch_inputs))\n",
    "        outputs.append(torch.stack(batch_outputs))\n",
    "        \n",
    "        average_loss = average_loss / i # use len(dataloader) for full batches\n",
    "        average_reconstruction_loss = average_reconstruction_loss / i # use len(dataloader) for full batches\n",
    "        average_prediction_loss = average_prediction_loss / i # use len(dataloader) for full batches\n",
    "\n",
    "        losses.append(average_loss) \n",
    "        reconstruction_losses.append(average_reconstruction_loss)\n",
    "        prediction_losses.append(average_prediction_loss)\n",
    "        print(\"epoch : {}/{} | Loss = {:,.4f} | Reconstruction Loss: {:,.4f} | Prediction Loss: {:,.4f}\".format(epoch+1, num_epochs, average_loss, average_reconstruction_loss, average_prediction_loss))\n",
    "        \n",
    "        if epoch == 0 and not checkpoint_loaded: best_loss = average_loss\n",
    "        elif epoch == 0 and checkpoint_loaded: best_loss = min(average_loss, loss_checkpoint)\n",
    "            \n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            torch.save({\n",
    "             'epoch': epoch,\n",
    "             'model_state_dict': model.state_dict(),\n",
    "             'optimizer_state_dict': optimizer.state_dict(),\n",
    "             'loss': best_loss,\n",
    "             }, checkpoint_path)\n",
    "            print(\"Better loss achieved -- saved model checkpoint to {}.\".format(checkpoint_path))\n",
    "            \n",
    "    return losses, reconstruction_losses, prediction_losses, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, reconstruction_losses, prediction_losses, inputs, outputs = train(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load losses & predictions from pickle/json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = json.load(open(\"./logs/vae_53joints_seqlen49_pred0/losses.json\"))\n",
    "losses = dict['overall_losses']\n",
    "reconstruction_losses = dict['reconstruction_losses']\n",
    "prediction_losses = dict['prediction_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.load(\"./logs/vae_53joints_seqlen49_pred0/train_inputs.npy\")\n",
    "outputs = np.load(\"./logs/vae_53joints_seqlen49_pred0/train_outputs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# ax.plot(np.arange(len(losses)), losses, label=\"Total\")\n",
    "ax.plot(np.arange(len(losses)), reconstruction_losses, label=\"Reconstruction\")\n",
    "# ax.plot(np.arange(len(losses)), prediction_losses, label=\"Prediction\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=16)\n",
    "ax.set_ylabel(\"Loss\", fontsize=16)\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_ylim(0,1)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "ax.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input_batch = inputs[0]\n",
    "n_joints = int(first_input_batch.shape[0]/32)\n",
    "first_input_seq = first_input_batch[:n_joints, :]\n",
    "\n",
    "# reshape to be n_joints x n_timesteps x n_dim\n",
    "first_input_seq = first_input_seq.reshape((first_input_seq.shape[0],int(first_input_seq.shape[1]/3),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_predicted_batch = outputs[0]\n",
    "n_joints = int(first_predicted_batch.shape[0]/32)\n",
    "first_predicted_seq = first_predicted_batch[:n_joints, :]\n",
    "\n",
    "# reshape to be n_joints x n_timesteps x n_dim\n",
    "first_predicted_seq = first_predicted_seq.reshape((first_predicted_seq.shape[0],int(first_predicted_seq.shape[1]/3),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "for joint in range(1): # first few joints\n",
    "# for joint in range(first_seq.shape[0]): # all joints\n",
    "    # plot x & y for the sequence\n",
    "    plt.plot(first_input_seq[joint,:,0], first_input_seq[joint,:,1], 'o--', label=\"Input Joint \"+str(joint)) \n",
    "    plt.plot(first_predicted_seq[joint,:,0], first_predicted_seq[joint,:,1], 'o--', label=\"Predicted Joint \"+str(joint)) \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up next:\n",
    "- Try to overfit the input data (loss = 0) by setting prediction weight = 0\n",
    "- Weight the prediction loss to ensure that the most immediate steps are more important to reconstruct than far future steps (like 1/2^t or something)\n",
    "- Look at VAE outputs! \n",
    "- Look at NRI outputs!\n",
    "\n",
    "### For later:\n",
    "- The Gaussian negative log likelihood loss functions will only make sense when the output of the decoder is mu (eq'n 16 & 17)\n",
    "\n",
    "### Done\n",
    "- ~~Predict 50 + k timesteps w/ separate MSE losses~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             my_nll_loss = gaussian_neg_log_likelihood(x=batch.x, mu=output, sigma=sigma)\n",
    "#             nll_loss = nll_gaussian(preds=output, target=batch.x.to(device), variance=5e-5)\n",
    "#             kl_loss = kl_categorical_uniform(torch.exp(log_probabilities), data[0].num_nodes, num_edge_types, add_const=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
