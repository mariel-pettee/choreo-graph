{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, SubsetRandomSampler, SequentialSampler\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "import networkx as nx # for visualizing graphs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from functions.load_data import MarielDataset, edges\n",
    "from functions.functions import *\n",
    "from functions.modules import *\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 10\n",
    "predicted_timesteps = 0\n",
    "data = MarielDataset(seq_len=seq_len, reduced_joints=False, predicted_timesteps=predicted_timesteps, no_overlap=True)\n",
    "\n",
    "train_indices = np.arange(int(0.7*len(data))) # 70% split for training data, no shuffle\n",
    "val_indices = np.arange(int(0.7*len(data)),int(0.85*len(data))) # next 15% on validation\n",
    "test_indices = np.arange(int(0.85*len(data)), len(data)) # last 15% on test\n",
    "\n",
    "dataloader_train = DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=True, sampler=SequentialSampler(train_indices))\n",
    "dataloader_val = DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=True, sampler=SequentialSampler(val_indices))\n",
    "dataloader_test = DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=True, sampler=SequentialSampler(test_indices))\n",
    "\n",
    "print(\"\\nGenerated {:,} batches of shape: {}\".format(len(dataloader_train), data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR load from saved dataloaders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.load(\"./logs/nooverlap_53joints_seqlen10_pred0/dataloader_train.pth\")\n",
    "dataloader_val = torch.load(\"./logs/nooverlap_53joints_seqlen10_pred0/dataloader_val.pth\")\n",
    "dataloader_test = torch.load(\"./logs/nooverlap_53joints_seqlen10_pred0/dataloader_test.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = 30 # data.seq_len*data.n_dim\n",
    "edge_features = 10 # data[0].num_edge_features\n",
    "node_embedding_dim = 25\n",
    "edge_embedding_dim = 4 # number of edge types\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "seq_len = 10\n",
    "predicted_timesteps = 0\n",
    "checkpoint_loaded = False \n",
    "\n",
    "model = VAE(node_features=node_features, \n",
    "            edge_features=edge_features, \n",
    "            hidden_size=hidden_size, \n",
    "            node_embedding_dim=node_embedding_dim,\n",
    "            edge_embedding_dim=edge_embedding_dim,\n",
    "            num_layers=num_layers,\n",
    "            input_size=node_embedding_dim, \n",
    "            output_size=node_features+predicted_timesteps*3,\n",
    "           )\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-4, weight_decay=5e-4)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(\"Total trainable parameters: {:,}\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: load pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the whole model + weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"weights/seqlen3_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR load the model state into the pre-existing model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./logs/nooverlap_53joints_seqlen10_pred0/best_weights.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss_checkpoint = checkpoint['loss']\n",
    "checkpoint_loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "prediction_to_reconstruction_loss_ratio = 0 # you might want to weight the prediction loss higher to help it compete with the larger prediction seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "    prediction_to_reconstruction_loss_ratio = 0\n",
    "    total_test_loss = 0\n",
    "    total_test_reco_loss = 0\n",
    "    total_test_pred_loss = 0\n",
    "    n_batches = 0\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader_test, desc=\"Test batches\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        ### CALCULATE MODEL OUTPUTS\n",
    "        output = model(batch)\n",
    "        \n",
    "        ### CALCULATE LOSS\n",
    "        test_reco_loss = mse_loss(batch.x.to(device), output[:,:node_features]) # compare first seq_len timesteps\n",
    "        if predicted_timesteps > 0: \n",
    "            test_pred_loss = prediction_to_reconstruction_loss_ratio*mse_loss(batch.y.to(device), output[:,node_features:]) # compare last part to unseen data\n",
    "            test_loss = test_reco_loss + test_pred_loss\n",
    "        else:\n",
    "            test_loss = test_reco_loss\n",
    "\n",
    "        ### ADD LOSSES TO TOTALS\n",
    "        total_test_loss += test_loss.item()\n",
    "        total_test_reco_loss += test_reco_loss.item()\n",
    "#         if args.predicted_timesteps > 0: \n",
    "#             total_test_pred_loss += test_pred_loss.item()\n",
    "        n_batches += 1\n",
    "            \n",
    "    ### CALCULATE AVERAGE LOSSES PER EPOCH   \n",
    "    average_test_loss = total_test_loss / n_batches\n",
    "    average_test_reco_loss = total_test_reco_loss / n_batches\n",
    "    average_test_pred_loss = total_test_pred_loss / n_batches\n",
    "    print(\"Loss = {:,.4f} | Reconstruction Loss: {:,.4f} | Prediction Loss: {:,.4f}\".format(average_test_loss, \n",
    "                                                                                            average_test_reco_loss, \n",
    "                                                                                            average_test_pred_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load losses & predictions from pickle/json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = json.load(open(\"./logs/vae_53joints_seqlen49_pred0/losses.json\"))\n",
    "losses = dict['overall_losses']\n",
    "reconstruction_losses = dict['reconstruction_losses']\n",
    "prediction_losses = dict['prediction_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.load(\"./logs/vae_53joints_seqlen49_pred0/train_inputs.npy\")\n",
    "outputs = np.load(\"./logs/vae_53joints_seqlen49_pred0/train_outputs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# ax.plot(np.arange(len(losses)), losses, label=\"Total\")\n",
    "ax.plot(np.arange(len(losses)), reconstruction_losses, label=\"Reconstruction\")\n",
    "# ax.plot(np.arange(len(losses)), prediction_losses, label=\"Prediction\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=16)\n",
    "ax.set_ylabel(\"Loss\", fontsize=16)\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_ylim(0,1)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "ax.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input_batch = inputs[0]\n",
    "n_joints = int(first_input_batch.shape[0]/32)\n",
    "first_input_seq = first_input_batch[:n_joints, :]\n",
    "\n",
    "# reshape to be n_joints x n_timesteps x n_dim\n",
    "first_input_seq = first_input_seq.reshape((first_input_seq.shape[0],int(first_input_seq.shape[1]/3),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_predicted_batch = outputs[0]\n",
    "n_joints = int(first_predicted_batch.shape[0]/32)\n",
    "first_predicted_seq = first_predicted_batch[:n_joints, :]\n",
    "\n",
    "# reshape to be n_joints x n_timesteps x n_dim\n",
    "first_predicted_seq = first_predicted_seq.reshape((first_predicted_seq.shape[0],int(first_predicted_seq.shape[1]/3),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "for joint in range(1): # first few joints\n",
    "# for joint in range(first_seq.shape[0]): # all joints\n",
    "    # plot x & y for the sequence\n",
    "    plt.plot(first_input_seq[joint,:,0], first_input_seq[joint,:,1], 'o--', label=\"Input Joint \"+str(joint)) \n",
    "    plt.plot(first_predicted_seq[joint,:,0], first_predicted_seq[joint,:,1], 'o--', label=\"Predicted Joint \"+str(joint)) \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up next:\n",
    "- Try to overfit the input data (loss = 0) by setting prediction weight = 0\n",
    "- Weight the prediction loss to ensure that the most immediate steps are more important to reconstruct than far future steps (like 1/2^t or something)\n",
    "- Look at VAE outputs! \n",
    "- Look at NRI outputs!\n",
    "\n",
    "### For later:\n",
    "- The Gaussian negative log likelihood loss functions will only make sense when the output of the decoder is mu (eq'n 16 & 17)\n",
    "\n",
    "### Done\n",
    "- ~~Predict 50 + k timesteps w/ separate MSE losses~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             my_nll_loss = gaussian_neg_log_likelihood(x=batch.x, mu=output, sigma=sigma)\n",
    "#             nll_loss = nll_gaussian(preds=output, target=batch.x.to(device), variance=5e-5)\n",
    "#             kl_loss = kl_categorical_uniform(torch.exp(log_probabilities), data[0].num_nodes, num_edge_types, add_const=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
