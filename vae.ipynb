{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, SubsetRandomSampler, SequentialSampler\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "import networkx as nx # for visualizing graphs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from functions.load_data import MarielDataset, edges\n",
    "from functions.functions import *\n",
    "from functions.modules import *\n",
    "from functions.seq_autoencoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./logs/nri_gpu_noskip\"\n",
    "dataloader_test = torch.load(os.path.join(folder,\"dataloader_test.pth\"))\n",
    "checkpoint_path = os.path.join(folder,\"best_weights.pth\")\n",
    "args_file = os.path.join(folder, 'args.pkl')\n",
    "args = pickle.load(open(args_file, \"rb\" ))['args']\n",
    "checkpoint_loaded = False \n",
    "\n",
    "# Load these if training actually completed:\n",
    "if os.path.exists(os.path.join(folder,\"losses.json\")):\n",
    "    dict = json.load(open(os.path.join(folder,\"losses.json\")))\n",
    "    train_losses = dict['train_losses']\n",
    "    val_losses = dict['val_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and args.no_cuda == False:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "    \n",
    "device = 'cpu'\n",
    "\n",
    "model = NRI(device=device,\n",
    "            node_features=args.seq_len*6, \n",
    "            edge_features=args.seq_len, \n",
    "            hidden_size=args.hidden_size, \n",
    "            node_embedding_dim=args.node_embedding_dim,\n",
    "            edge_embedding_dim=args.edge_embedding_dim,\n",
    "            skip_connection=args.skip_connection,\n",
    "            dynamic_graph=args.dynamic_graph,\n",
    "            seq_len=args.seq_len,\n",
    "           )\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=args.lr, weight_decay=5e-4)\n",
    "\n",
    "print(\"Using {}\".format(device))\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "print(\"Total trainable parameters: {:,}\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss_checkpoint = checkpoint['loss']\n",
    "checkpoint_loaded = True\n",
    "n_joints = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batch_limit=0):\n",
    "    mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "    prediction_to_reconstruction_loss_ratio = 1 # CHANGE THIS TO ARGS\n",
    "    total_test_loss = 0\n",
    "    total_test_reco_loss = 0\n",
    "    total_test_pred_loss = 0\n",
    "    n_batches = 0\n",
    "    actuals = []\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader_test, desc=\"Test batches\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        ### CALCULATE MODEL OUTPUTS\n",
    "        output, edges = model(batch)\n",
    "        \n",
    "        ### SAVE FOR ANIMATIONS\n",
    "        actuals.append(batch.x.detach().cpu().numpy())\n",
    "        preds.append(output.detach().cpu().numpy())\n",
    "\n",
    "        ### CALCULATE LOSS\n",
    "        test_loss = mse_loss(batch.x.to(device), output) # compare first seq_len timesteps\n",
    "\n",
    "        ### ADD LOSSES TO TOTALS\n",
    "        total_test_loss += test_loss.item()\n",
    "            \n",
    "        ### OPTIONAL -- STOP TESTING EARLY\n",
    "        n_batches += 1\n",
    "        if (batch_limit > 0) and (n_batches >= batch_limit): break # temporary -- for stopping training early\n",
    "\n",
    "    ### CALCULATE AVERAGE LOSSES PER EPOCH   \n",
    "    average_test_loss = total_test_loss / n_batches\n",
    "    print(\"Loss = {:,.4f}\".format(average_test_loss))\n",
    "    return actuals, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals, preds = test(batch_limit = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number = 1\n",
    "truth_sequences = []\n",
    "predicted_sequences = []\n",
    "\n",
    "for seq_number in np.arange(args.batch_size):\n",
    "    actual = actuals[batch_number][seq_number*n_joints:seq_number*n_joints+n_joints].reshape((n_joints,args.seq_len,6))[:,:,:3] # take the first 3 dimensions for positions, not velocities\n",
    "    pred = preds[batch_number][seq_number*n_joints:seq_number*n_joints+n_joints].reshape((n_joints,args.seq_len,6))[:,:,:3]\n",
    "    actual = np.transpose(actual, [1,0,2])\n",
    "    pred = np.transpose(pred, [1,0,2])\n",
    "    truth_sequences.append(actual)\n",
    "    predicted_sequences.append(pred)\n",
    "    \n",
    "truth_sequences = np.asarray(truth_sequences).reshape((args.batch_size*args.seq_len, n_joints, 3))\n",
    "predicted_sequences = np.asarray(predicted_sequences).reshape((args.batch_size*args.seq_len, n_joints, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "timesteps = args.seq_len*args.batch_size\n",
    "# timesteps = args.seq_len\n",
    "animation = animate_stick(truth_sequences[start_index:start_index+timesteps,:,:], \n",
    "                          ghost=predicted_sequences[start_index:start_index+timesteps,:,:], \n",
    "                          ghost_shift=0.4,\n",
    "                          ax_lims = (-0.7,0.7),\n",
    "                          figsize=(10,8), cmap='inferno')\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_recurrent = [0.0969,0.0103,0.0100,0.0067,0.0066,0.0063,0.0056,0.0051,0.0042,0.0041,0.0048,0.0027,0.0016,0.0037,0.0038,0.0031,0.0036,0.0017,0.0016,0.0041,0.0089,]\n",
    "nosampling_recurrent = [0.6768,0.3889,0.0499,0.0085,0.0081,0.0079,0.0080,0.4788,0.0412,0.1520,0.0554,0.0090,0.0088,0.0088,0.0088,223.3473,0.0089,0.0186,0.0092,0.0089,0.0093,]\n",
    "sampling_norecurrent = [6,419,004.4403,1,180,173.9217,12,472.5292,5,309.1666,3,601.1410,23,310.8198,10,171,718.4292,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,962.9072,]\n",
    "nosampling_norecurrent = [16,822,800.1428,651,681.9966,187,729.5295,211,615.4727,334,911.2555,183,381.5607,28,994.0494,28,142.1309,28,039.8979,27,708.8960,26,805.5452,23,027.8847,10,154.9895,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,0.0089,]\n",
    "val_losses_moreparams = [1.2585,0.2413,0.2063,0.3304,0.1753,0.1665,0.1516,0.2043,0.1444,32.0709,0.1563,15.5246,0.7867,1.8139,2.1597,0.1700,0.1611,0.1606,0.1605,0.1671,0.1589,0.7795,0.1553,0.1550,0.1546,0.1544,0.1546,0.1546,0.1522,0.1533,0.1545,0.1509,0.1521,0.1538,0.1538,0.1467,0.1402,0.1433,0.1413,0.1402,0.1425,0.1419,0.1372,0.1359,0.1319,0.1331,0.1354,0.1371,0.1360,0.1283,0.1283,0.1277,0.1277,0.1271,0.1353,0.1353,0.1390,0.1249,0.0118,0.0040,0.1584,0.1568,0.1558,0.1556,0.1556,0.1554,0.1536,0.1535,0.1533,0.1523,0.1419,0.1338,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# ax.plot(np.arange(len(val_losses)), val_losses, label=\"Graph VAE (250k params)\")\n",
    "# ax.plot(np.arange(len(val_losses_moreparams)), val_losses_moreparams, label=\"Graph VAE (1.8M params)\")\n",
    "# ax.plot(np.arange(len(sampling_recurrent)), sampling_recurrent, label=\"Sampling, Recurrent\")\n",
    "# ax.plot(np.arange(len(nosampling_recurrent)), nosampling_recurrent, label=\"No Sampling, Recurrent\")\n",
    "ax.plot(np.arange(len(sampling_norecurrent)), sampling_norecurrent, label=\"Sampling, No Recurrent\")\n",
    "ax.plot(np.arange(len(nosampling_norecurrent)), nosampling_norecurrent, label=\"No Sampling, No Recurrent\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=16)\n",
    "ax.set_ylabel(\"Validation Reco Loss\", fontsize=16)\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_ylim(0,0.1)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "ax.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up next:\n",
    "- Try to overfit the input data (loss = 0) by setting prediction weight = 0\n",
    "- Weight the prediction loss to ensure that the most immediate steps are more important to reconstruct than far future steps (like 1/2^t or something)\n",
    "\n",
    "### For later:\n",
    "- The Gaussian negative log likelihood loss functions will only make sense when the output of the decoder is mu (eq'n 16 & 17)\n",
    "\n",
    "### Done\n",
    "- ~~Predict 50 + k timesteps w/ separate MSE losses~~\n",
    "- ~~Look at VAE outputs!~~\n",
    "- ~~Look at NRI outputs!~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
