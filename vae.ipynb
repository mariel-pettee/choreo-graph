{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "import networkx as nx # for visualizing graphs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functions.load_data import MarielDataset, edges\n",
    "from functions.functions import *\n",
    "from functions.modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder Procedure\n",
    "1. Create an embedding of the node features (H) using an MLP\n",
    "2. Create a message to pass through the edges of the graph using the embedded node features (H) and another MLP\n",
    "3. Aggregate the messages created in Step 2 for each node to update node features\n",
    "4. Pass the updated node features through another MLP to get the \"Pre-posterior\"; the posterior then becomes the softmax of the pre-posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numpy dataset contains 38,309 timesteps of 53 joints with 3 dimensions each.\n",
      "\n",
      "Generating overlapping sequences...\n",
      "Using (x,y)-centering...\n",
      "Reducing joints...\n",
      "\n",
      "Generated 1,197 batches of shape: Data(edge_attr=[648, 10], edge_index=[2, 648], x=[18, 30], y=[18, 30])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seq_len = 10\n",
    "data = MarielDataset(seq_len=seq_len, reduced_joints=True)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "print(\"\\nGenerated {:,} batches of shape: {}\".format(len(dataloader), data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "MLPEncoder(\n",
      "  (node_embedding): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (edge_embedding): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (MLP): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=5000, bias=True)\n",
      "  )\n",
      "  (graph_conv_1): MLPGraphConv(50, 50)\n",
      "  (graph_conv_2): MLPGraphConv(50, 50)\n",
      "  (graph_conv_list): ModuleList(\n",
      "    (0): MLPGraphConv(50, 50)\n",
      "    (1): MLPGraphConv(50, 50)\n",
      "  )\n",
      ")\n",
      "RNNDecoder(\n",
      "  (node_transform): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (graph_conv): GatedGraphConv(30, num_layers=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "node_features = data.seq_len*data.n_dim\n",
    "edge_features = data[0].num_edge_features\n",
    "node_embedding_dim = 50\n",
    "hidden_size = 50\n",
    "edge_embedding_dim = 10\n",
    "num_layers = 10\n",
    "num_edge_types = 4\n",
    "\n",
    "encoder = MLPEncoder(node_features=node_features, \n",
    "                   edge_features=edge_features, \n",
    "                   hidden_size=hidden_size, \n",
    "                   node_embedding_dim=node_embedding_dim,\n",
    "                   edge_embedding_dim=edge_embedding_dim)\n",
    "\n",
    "decoder = RNNDecoder(input_size=node_embedding_dim, \n",
    "                    hidden_size=hidden_size, \n",
    "                    output_size=node_features,\n",
    "                    edge_embedding_dim=edge_embedding_dim,\n",
    "                    num_layers=num_layers\n",
    "                    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.01, weight_decay=5e-4)\n",
    "loss_func = torch.nn.MSELoss(reduction='mean')\n",
    "# dummy_matrix = torch.tensor(np.random.rand(batch_size*data[0].num_nodes,node_features), dtype=torch.float)\n",
    "# dummy_matrix = dummy_matrix.to(device)\n",
    "\n",
    "def train(num_epochs):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        i = 0\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad() # reset the gradients to zero\n",
    "\n",
    "            ### ENCODER\n",
    "            if torch.isnan(encoder.node_embedding[0].weight).any(): \n",
    "                print(\"NaNs detected in the node embedding weights BEFORE encoder!\")\n",
    "                break\n",
    "            node_embedding, edge_index, edge_embedding = encoder(batch)\n",
    "            if torch.isnan(encoder.node_embedding[0].weight).any(): \n",
    "                print(\"NaNs detected in the node embedding weights AFTER encoder!\")\n",
    "                break\n",
    "                \n",
    "            ### SAMPLING\n",
    "            z_soft = gumbel_softmax(edge_embedding, tau=0.5, hard=False)\n",
    "            probability = my_softmax(edge_embedding, -1)\n",
    "            \n",
    "#             print(\"z_soft:\",z_soft.shape,z_soft[0])\n",
    "#             print(\"Prob:\",prob.shape,prob[0])\n",
    "            \n",
    "            ### What dimensionality should this multivariate Gaussian be? 1 distribution per edge embedding dim?\n",
    "#             distribution = MultivariateNormal(edge_embedding.mean(dim=0).cpu(), torch.eye(edge_embedding_dim)) # mean & covariance matrix\n",
    "#             z = distribution.sample()\n",
    "#             z_soft_other = F.softmax(z, dim=-1).to(device) # is dim=-1 necessary if I have dim=0 above?\n",
    "#             print(\"z_soft_other:\",z_soft_other.shape)\n",
    "            \n",
    "            ### DECODER\n",
    "            output = decoder(node_embedding, edge_index, edge_embedding, z_soft)\n",
    "\n",
    "            ### CALCULATE LOSS\n",
    "#             loss = loss_func()\n",
    "            print(\"Predictions:\",output[0,:3])\n",
    "            print(\"Maximum prediction:\",torch.max(output).item())\n",
    "#             print(torch.sum((output-batch.x.to(device))**2))\n",
    "            nll_loss = nll_gaussian(preds=output, target=batch.x.to(device), variance=5e-5)\n",
    "            kl_loss = kl_categorical_uniform(probability, data[0].num_nodes, num_edge_types)\n",
    "            batch_loss = nll_loss + kl_loss \n",
    "            print(\"NLL Loss: {:.5f} | KL Loss: {:.5f}\".format(nll_loss, kl_loss))\n",
    "            print(\"Batch {}, Loss: {:.5f}\".format(i, batch_loss.item()))\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(batch_loss.item())\n",
    "            loss += batch_loss.item()\n",
    "            i += 1\n",
    "        loss = loss / len(dataloader)\n",
    "        print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch, num_epochs, loss))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([-0.9998,  1.0000, -0.9833], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8768.97363 | KL Loss: -0.00027\n",
      "Batch 0, Loss: 8768.97363\n",
      "Predictions: tensor([-0.2772, -0.8800,  0.9992], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 0.9999995231628418\n",
      "NLL Loss: 7815.20459 | KL Loss: -0.00027\n",
      "Batch 1, Loss: 7815.20410\n",
      "Predictions: tensor([ 1.0000, -0.9541, -0.8143], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 7966.53027 | KL Loss: -0.00027\n",
      "Batch 2, Loss: 7966.52979\n",
      "Predictions: tensor([ 1., -1.,  1.], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1194366517379072.0\n",
      "NLL Loss: 26416615269254981860807160102912.00000 | KL Loss: -0.00027\n",
      "Batch 3, Loss: 26416615269254981860807160102912.00000\n",
      "Predictions: tensor([ 1., -1.,  1.], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1194366517379072.0\n",
      "NLL Loss: 26416593508590228797482015391744.00000 | KL Loss: -0.00027\n",
      "Batch 4, Loss: 26416593508590228797482015391744.00000\n",
      "Predictions: tensor([ 1., -1.,  1.], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1311621808914432.0\n",
      "NLL Loss: 31857913012087972439892477083648.00000 | KL Loss: -0.00027\n",
      "Batch 5, Loss: 31857913012087972439892477083648.00000\n",
      "Predictions: tensor([ 1., -1.,  1.], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1311621272043520.0\n",
      "NLL Loss: 31857985547637149317642959454208.00000 | KL Loss: -0.00027\n",
      "Batch 6, Loss: 31857985547637149317642959454208.00000\n",
      "Predictions: tensor([-0.0710, -1.0000, -0.9777], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8583.82715 | KL Loss: -0.00027\n",
      "Batch 7, Loss: 8583.82715\n",
      "Predictions: tensor([ 0.9993, -0.9808, -0.9999], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 9752.48730 | KL Loss: -0.00027\n",
      "Batch 8, Loss: 9752.48730\n",
      "Predictions: tensor([ 0.9232, -0.9996, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 9117.17676 | KL Loss: -0.00027\n",
      "Batch 9, Loss: 9117.17676\n",
      "Predictions: tensor([ 1., -1.,  1.], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1377073889280000.0\n",
      "NLL Loss: 35116988429341152812749073416192.00000 | KL Loss: -0.00027\n",
      "Batch 10, Loss: 35116988429341152812749073416192.00000\n",
      "Predictions: tensor([-0.9998,  1.0000,  0.9645], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 0.9999971389770508\n",
      "NLL Loss: 7953.16455 | KL Loss: -0.00027\n",
      "Batch 11, Loss: 7953.16406\n",
      "Predictions: tensor([-0.9933, -1.0000, -0.9561], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 9642.17969 | KL Loss: -0.00027\n",
      "Batch 12, Loss: 9642.17969\n",
      "Predictions: tensor([ 1.0000, -0.9173, -0.1502], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 7437.86816 | KL Loss: -0.00027\n",
      "Batch 13, Loss: 7437.86768\n",
      "Predictions: tensor([ 1., -1.,  1.], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.21658706755584e+16\n",
      "NLL Loss: 2740894568360856902907828264501248.00000 | KL Loss: -0.00027\n",
      "Batch 14, Loss: 2740894568360856902907828264501248.00000\n",
      "Predictions: tensor([-0.9999, -0.9788,  1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8736.20801 | KL Loss: -0.00027\n",
      "Batch 15, Loss: 8736.20801\n",
      "Predictions: tensor([-0.9902, -0.8989, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8954.48828 | KL Loss: -0.00027\n",
      "Batch 16, Loss: 8954.48828\n",
      "Predictions: tensor([-1.0000,  0.9523, -0.4318], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8261.68652 | KL Loss: -0.00027\n",
      "Batch 17, Loss: 8261.68652\n",
      "Predictions: tensor([-0.9995,  0.9906,  0.9998], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8519.91309 | KL Loss: -0.00027\n",
      "Batch 18, Loss: 8519.91309\n",
      "Predictions: tensor([ 1.0000, -0.9998,  0.9913], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8329.09961 | KL Loss: -0.00027\n",
      "Batch 19, Loss: 8329.09961\n",
      "Predictions: tensor([ 0.9629, -1.0000, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8993.21582 | KL Loss: -0.00027\n",
      "Batch 20, Loss: 8993.21582\n",
      "Predictions: tensor([ 0.9993, -1.0000,  1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8478.93457 | KL Loss: -0.00027\n",
      "Batch 21, Loss: 8478.93457\n",
      "Predictions: tensor([ 0.9285, -1.0000,  0.8174], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 7348.06641 | KL Loss: -0.00027\n",
      "Batch 22, Loss: 7348.06592\n",
      "Predictions: tensor([ 1., -1., -1.], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 2.401280566965043e+16\n",
      "NLL Loss: inf | KL Loss: -0.00027\n",
      "Batch 23, Loss: inf\n",
      "Predictions: tensor([ 0.9955, -0.8595, -0.7220], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8440.56836 | KL Loss: -0.00027\n",
      "Batch 24, Loss: 8440.56836\n",
      "Predictions: tensor([-1.0000, -0.9999, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 9300.91602 | KL Loss: -0.00027\n",
      "Batch 25, Loss: 9300.91602\n",
      "Predictions: tensor([-0.9989, -0.9999,  0.9943], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8867.74023 | KL Loss: -0.00027\n",
      "Batch 26, Loss: 8867.74023\n",
      "Predictions: tensor([ 0.9998,  0.7536, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8004.00928 | KL Loss: -0.00027\n",
      "Batch 27, Loss: 8004.00879\n",
      "Predictions: tensor([-0.5168,  0.6689,  0.5098], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 7799.73145 | KL Loss: -0.00027\n",
      "Batch 28, Loss: 7799.73096\n",
      "Predictions: tensor([-1.0000,  1.0000,  1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 9340.11816 | KL Loss: -0.00027\n",
      "Batch 29, Loss: 9340.11816\n",
      "Predictions: tensor([ 0.9379, -1.0000,  0.9959], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 9491.82715 | KL Loss: -0.00027\n",
      "Batch 30, Loss: 9491.82715\n",
      "Predictions: tensor([ 0.9994, -0.9997, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8848.19629 | KL Loss: -0.00027\n",
      "Batch 31, Loss: 8848.19629\n",
      "Predictions: tensor([-0.5088, -0.9990, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8786.38086 | KL Loss: -0.00027\n",
      "Batch 32, Loss: 8786.38086\n",
      "Predictions: tensor([ 0.9999, -1.0000, -0.6853], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8377.63086 | KL Loss: -0.00027\n",
      "Batch 33, Loss: 8377.63086\n",
      "Predictions: tensor([ 0.9998,  0.1774, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8788.29004 | KL Loss: -0.00027\n",
      "Batch 34, Loss: 8788.29004\n",
      "Predictions: tensor([ 0.9999,  0.9340, -1.0000], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Maximum prediction: 1.0\n",
      "NLL Loss: 8629.35645 | KL Loss: -0.00027\n",
      "Batch 35, Loss: 8629.35645\n"
     ]
    }
   ],
   "source": [
    "losses = train(num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(np.arange(len(losses)), losses)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=16)\n",
    "ax.set_ylabel(\"Loss\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still missing: \n",
    "- How do we use edge_embedding in the decoder? (Edge weight?)\n",
    "- Gumbel softmax\n",
    "\n",
    "\n",
    "- NLL Loss vs MSE Loss\n",
    "- NaNs in the node embedding layer with higher-dim inputs? -- does reducing MSELoss w/ either sum or mean help?\n",
    "- Adding more layers to node embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
